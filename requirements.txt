accelerate
codetiming
datasets
liger-kernel
mathruler
numpy
omegaconf
pandas
peft
pillow
pyarrow>=15.0.0
pylatexenc
qwen-vl-utils
ray[default]
tensordict
torchdata
transformers==4.51.2
vllm==0.8.4
wandb
tenacity
math_verify
jieba
nltk
Levenshtein
tensorboard
torch==2.6.0
flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl